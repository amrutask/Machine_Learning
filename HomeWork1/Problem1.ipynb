{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Problem1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amrutask/Machine_Learning/blob/master/HomeWork1/Problem1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "CqiUNMXz19-p",
        "colab_type": "code",
        "outputId": "fc230468-0d0a-419f-badd-3a2fc8541c44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import h5py\n",
        "import scipy\n",
        "from PIL import Image\n",
        "from scipy import ndimage\n",
        "%matplotlib inline\n",
        "import keras\n",
        "from keras.datasets import mnist"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "KfbP1x7t2SG0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class logistic_regression():\n",
        "  \n",
        "  def __init__(self, size):\n",
        "    \n",
        "    self.weights = np.zeros((size,1))\n",
        "    self.bias = 0.0\n",
        "    self.accuracy=0.0\n",
        "      \n",
        "  \n",
        "  def train_and_optimize(self, train_x, train_y, learning_rate=0.01, num_iters=50, mini_batch_size=200):\n",
        "    \n",
        "    w = np.zeros((train_x.shape[0],1))\n",
        "    b=0.0\n",
        "    m = train_x.shape[1]   #no of training smaples\n",
        "    print(\"No of training examples:\", m)\n",
        "    num_batches=int(m/mini_batch_size)\n",
        "    costs=[] \n",
        "    \n",
        "    #mini batch gradient descent\n",
        "    for i in range(num_iters):\n",
        "      \n",
        "      batch_cost=[]\n",
        "      shuffled_indices = np.random.permutation(m)       \n",
        "      train_x = train_x[:,shuffled_indices]\n",
        "      train_y = train_y[shuffled_indices]\n",
        "      start=0\n",
        "      end=mini_batch_size-1\n",
        "      \n",
        "      for j in range(num_batches):\n",
        "        X=train_x[:, start:end]\n",
        "        Y=train_y[start:end]\n",
        "        \n",
        "        A = 1/(1 + np.exp(-((np.dot(w.transpose(),X))+ b)))  # compute activation\n",
        "        #Mean Squared Error\n",
        "        cost = (1/mini_batch_size) *  np.sum(np.dot((A-Y).transpose(),(A-Y)))\n",
        "        \n",
        "        dz=np.dot(A.transpose(),(1-A))\n",
        "        temp=np.dot(X, dz)\n",
        "        dw = (1/mini_batch_size) * np.dot(temp,(A-Y).transpose())\n",
        "        db = (1/mini_batch_size) * np.sum(np.dot(dz,(A-Y).transpose()))\n",
        "        cost = np.squeeze(cost)\n",
        "  \n",
        "        w = w - learning_rate * dw\n",
        "        b = b - learning_rate * db\n",
        "        \n",
        "        batch_cost.append(cost)\n",
        "        start=start+mini_batch_size\n",
        "        end=end+mini_batch_size\n",
        "        \n",
        "      costs.append(sum(batch_cost) / len(batch_cost))\n",
        "    \n",
        "      if i%25==0 or i==num_iters-1:\n",
        "        print(\"Cost after {} iterations: {}\".format(i, sum(costs)/len(costs)))\n",
        "    \n",
        "    self.weights=w\n",
        "    self.bias=b\n",
        "    \n",
        " \n",
        "  def predict(self, test_x, Label):\n",
        "  \n",
        "    A = 1/(1 + np.exp(-((np.dot(self.weights.transpose(),test_x))+ self.bias)))\n",
        "    m = A.shape[1]\n",
        "    Y_pred = np.zeros((1, m))\n",
        "  \n",
        "    for i in range(A.shape[1]):\n",
        "      # Convert probabilities A to actual predictions\n",
        "        \n",
        "      if A[0,i]<= 0.5 :\n",
        "        Y_pred[0, i] = 0\n",
        "      else :\n",
        "        Y_pred[0, i] = 1\n",
        "  \n",
        "    self.accuracy=((np.sum(Y_pred==Label))/m) * 100\n",
        "    \n",
        "    return(self.accuracy)\n",
        "  \n",
        "  \n",
        "  def calc_probability(self, image_features):\n",
        "    \n",
        "    prob=1/(1 + np.exp(-((np.dot(self.weights.transpose(),image_features))+ self.bias)))\n",
        "    \n",
        "    return(prob)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jHRGOBVr2oEX",
        "colab_type": "code",
        "outputId": "36d0701d-2ed5-4614-8603-3e8ed74fe591",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1495
        }
      },
      "cell_type": "code",
      "source": [
        "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "\n",
        "train_x_flat = x_train.reshape(x_train.shape[0],-1).T\n",
        "test_x_flat = x_test.reshape(x_test.shape[0],-1).T\n",
        "\n",
        "train_x, test_x = train_x_flat / 255.0, test_x_flat / 255.0\n",
        "print(\"Size of training data:\",train_x.shape)\n",
        "print(\"Size of test data:\",test_x.shape)\n",
        "\n",
        "#one hot encoding of the labels\n",
        "train_y= np.eye(10)[y_train]\n",
        "test_y = np.eye(10)[y_test]\n",
        "print(\"Size of training labels:\", train_y.shape)\n",
        "print(\"Size of test labels:\", test_y.shape)\n",
        "\n",
        "no_classes=10  #0 to 9 digits\n",
        "\n",
        "#one vs all approach\n",
        "digits=[]\n",
        "for i in range(no_classes):\n",
        "  lgr=logistic_regression(train_x.shape[0])\n",
        "  print(\"\\nTraining class {} vs others.....\".format(i))\n",
        "  lgr.train_and_optimize(train_x, train_y[:,i])\n",
        "  acc=lgr.predict(test_x, test_y[:,i])\n",
        "  print(\"\\nAccuracy of predicting {} is: {}\".format(i, acc))\n",
        "  digits.append(lgr)\n",
        "  "
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of training data: (784, 60000)\n",
            "Size of test data: (784, 10000)\n",
            "Size of training labels: (60000, 10)\n",
            "Size of test labels: (10000, 10)\n",
            "\n",
            "Training class 0 vs others.....\n",
            "No of training examples: 60000\n",
            "Cost after 0 iterations: 0.7546375083460604\n",
            "Cost after 25 iterations: 0.11461020396875919\n",
            "Cost after 49 iterations: 0.0919236487911317\n",
            "\n",
            "Accuracy of predicting 0 is: 98.02\n",
            "\n",
            "Training class 1 vs others.....\n",
            "No of training examples: 60000\n",
            "Cost after 0 iterations: 0.8082383855278555\n",
            "Cost after 25 iterations: 0.13757280934701888\n",
            "Cost after 49 iterations: 0.11267435692531755\n",
            "\n",
            "Accuracy of predicting 1 is: 98.61999999999999\n",
            "\n",
            "Training class 2 vs others.....\n",
            "No of training examples: 60000\n",
            "Cost after 0 iterations: 0.7241385957447622\n",
            "Cost after 25 iterations: 0.11725292195349635\n",
            "Cost after 49 iterations: 0.09467699573569455\n",
            "\n",
            "Accuracy of predicting 2 is: 95.74000000000001\n",
            "\n",
            "Training class 3 vs others.....\n",
            "No of training examples: 60000\n",
            "Cost after 0 iterations: 0.7609194582658358\n",
            "Cost after 25 iterations: 0.11783989968972955\n",
            "Cost after 49 iterations: 0.09190460627931288\n",
            "\n",
            "Accuracy of predicting 3 is: 96.17999999999999\n",
            "\n",
            "Training class 4 vs others.....\n",
            "No of training examples: 60000\n",
            "Cost after 0 iterations: 0.5066907051782827\n",
            "Cost after 25 iterations: 0.10509775792045885\n",
            "Cost after 49 iterations: 0.08934124380228159\n",
            "\n",
            "Accuracy of predicting 4 is: 96.19\n",
            "\n",
            "Training class 5 vs others.....\n",
            "No of training examples: 60000\n",
            "Cost after 0 iterations: 0.46144880630129315\n",
            "Cost after 25 iterations: 0.10124835263660124\n",
            "Cost after 49 iterations: 0.08788283192006338\n",
            "\n",
            "Accuracy of predicting 5 is: 95.0\n",
            "\n",
            "Training class 6 vs others.....\n",
            "No of training examples: 60000\n",
            "Cost after 0 iterations: 0.6273382638151204\n",
            "Cost after 25 iterations: 0.11064069046284081\n",
            "Cost after 49 iterations: 0.09298812733382865\n",
            "\n",
            "Accuracy of predicting 6 is: 96.81\n",
            "\n",
            "Training class 7 vs others.....\n",
            "No of training examples: 60000\n",
            "Cost after 0 iterations: 0.5208522344157674\n",
            "Cost after 25 iterations: 0.10831619165309578\n",
            "Cost after 49 iterations: 0.09125668527951042\n",
            "\n",
            "Accuracy of predicting 7 is: 96.77\n",
            "\n",
            "Training class 8 vs others.....\n",
            "No of training examples: 60000\n",
            "Cost after 0 iterations: 0.7735137448409923\n",
            "Cost after 25 iterations: 0.1331839080233684\n",
            "Cost after 49 iterations: 0.10527157598892285\n",
            "\n",
            "Accuracy of predicting 8 is: 93.27\n",
            "\n",
            "Training class 9 vs others.....\n",
            "No of training examples: 60000\n",
            "Cost after 0 iterations: 0.9326606462769803\n",
            "Cost after 25 iterations: 0.1222962638669577\n",
            "Cost after 49 iterations: 0.09952653489977317\n",
            "\n",
            "Accuracy of predicting 9 is: 93.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "o_anf00_2wcA",
        "colab_type": "code",
        "outputId": "a38f2e68-a6c3-45b8-e3fd-23e44af0fda1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 670
        }
      },
      "cell_type": "code",
      "source": [
        "No_to_predict = input(\"Enter any image from 0 to 60000 for prediction: \")\n",
        "\n",
        "plt.imshow(train_x[:,int(No_to_predict)].reshape((28,28)))\n",
        "predictions=[]\n",
        "\n",
        "for i in range(no_classes):\n",
        "  lgr=digits[i]\n",
        "  prob=lgr.calc_probability(train_x[:,int(No_to_predict)])\n",
        "  print(\"\\nProbability of the number being {} is: {}\".format(i, prob))\n",
        "  predictions.append(prob)\n",
        "  \n",
        "preds=np.asarray(predictions)\n",
        "print(\"\\nPredicted number is:\",np.argmax(preds))\n",
        "  "
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter any image from 0 to 60000 for prediction: 7890\n",
            "\n",
            "Probability of the number being 0 is: [0.0063432]\n",
            "\n",
            "Probability of the number being 1 is: [0.00047228]\n",
            "\n",
            "Probability of the number being 2 is: [0.00336775]\n",
            "\n",
            "Probability of the number being 3 is: [0.00206402]\n",
            "\n",
            "Probability of the number being 4 is: [0.06258802]\n",
            "\n",
            "Probability of the number being 5 is: [0.00648606]\n",
            "\n",
            "Probability of the number being 6 is: [0.00285079]\n",
            "\n",
            "Probability of the number being 7 is: [0.18414813]\n",
            "\n",
            "Probability of the number being 8 is: [0.03239196]\n",
            "\n",
            "Probability of the number being 9 is: [0.64114603]\n",
            "\n",
            "Predicted number is: 9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAD4CAYAAADFJPs2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}