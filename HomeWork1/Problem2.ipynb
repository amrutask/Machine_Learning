{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Problem2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amrutask/Machine_Learning/blob/master/HomeWork1/Problem2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "cy7NV3JVc3u6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import h5py\n",
        "import scipy\n",
        "from PIL import Image\n",
        "from scipy import ndimage\n",
        "%matplotlib inline\n",
        "import keras\n",
        "from keras.datasets import mnist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LdNrXFElKHdo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class logistic_regression():\n",
        "  \n",
        "  def __init__(self, size):\n",
        "    \n",
        "    self.weights = np.zeros((size,1))\n",
        "    self.bias = 0.0\n",
        "    self.accuracy=0.0\n",
        "      \n",
        "  \n",
        "  def train_and_optimize(self, train_x, train_y, learning_rate=0.05, num_iters=50, mini_batch_size=200):\n",
        "    \n",
        "    w = np.zeros((train_x.shape[0],1))\n",
        "    b=0.0\n",
        "    m = train_x.shape[1]   #no of training smaples\n",
        "    print(\"No of training examples:\", m)\n",
        "    num_batches=int(m/mini_batch_size)\n",
        "    costs=[] \n",
        "    \n",
        "    #mini batch gradient descent\n",
        "    for i in range(num_iters):\n",
        "      \n",
        "      batch_cost=[]\n",
        "      shuffled_indices = np.random.permutation(m)       \n",
        "      train_x = train_x[:,shuffled_indices]\n",
        "      train_y = train_y[shuffled_indices]\n",
        "      start=0\n",
        "      end=mini_batch_size-1\n",
        "      \n",
        "      for j in range(num_batches):\n",
        "        X=train_x[:, start:end]\n",
        "        Y=train_y[start:end]\n",
        "        \n",
        "        A = 1/(1 + np.exp(-((np.dot(w.transpose(),X))+ b)))  # compute activation\n",
        "        cost = (-1/m) *  np.sum(np.multiply(Y, np.log(A)) + np.multiply((1-Y), np.log(1-A)))\n",
        "        dw = (1/m) * X.dot((A-Y).transpose())\n",
        "        db = (1/m) * np.sum((A-Y))\n",
        "        cost = np.squeeze(cost)\n",
        "  \n",
        "        w = w - learning_rate * dw\n",
        "        b = b - learning_rate * db\n",
        "        \n",
        "        batch_cost.append(cost)\n",
        "        start=start+mini_batch_size\n",
        "        end=end+mini_batch_size\n",
        "        \n",
        "      costs.append(sum(batch_cost) / len(batch_cost))\n",
        "    \n",
        "      if i%25==0 or i==num_iters-1:\n",
        "        print(\"Cost after {} iterations: {}\".format(i, sum(costs)/len(costs)))\n",
        "    \n",
        "    self.weights=w\n",
        "    self.bias=b\n",
        "    \n",
        " \n",
        "  def predict(self, test_x, Label):\n",
        "  \n",
        "    A = 1/(1 + np.exp(-((np.dot(self.weights.transpose(),test_x))+ self.bias)))\n",
        "    m = A.shape[1]\n",
        "    Y_pred = np.zeros((1, m))\n",
        "  \n",
        "    for i in range(A.shape[1]):\n",
        "      # Convert probabilities A to actual predictions\n",
        "        \n",
        "      if A[0,i]<= 0.5 :\n",
        "        Y_pred[0, i] = 0\n",
        "      else :\n",
        "        Y_pred[0, i] = 1\n",
        "  \n",
        "    self.accuracy=((np.sum(Y_pred==Label))/m) * 100\n",
        "    \n",
        "    return(self.accuracy)\n",
        "  \n",
        "  \n",
        "  def calc_probability(self, image_features):\n",
        "    \n",
        "    prob=1/(1 + np.exp(-((np.dot(self.weights.transpose(),image_features))+ self.bias)))\n",
        "    \n",
        "    return(prob)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aNIX_-BH16xb",
        "colab_type": "code",
        "outputId": "db82fe8b-1478-4995-b369-ccaecb323120",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1495
        }
      },
      "cell_type": "code",
      "source": [
        "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "\n",
        "train_x_flat = x_train.reshape(x_train.shape[0],-1).T\n",
        "test_x_flat = x_test.reshape(x_test.shape[0],-1).T\n",
        "\n",
        "train_x, test_x = train_x_flat / 255.0, test_x_flat / 255.0\n",
        "print(\"Size of training data:\",train_x.shape)\n",
        "print(\"Size of test data:\",test_x.shape)\n",
        "\n",
        "#one hot encoding of the labels\n",
        "train_y= np.eye(10)[y_train]\n",
        "test_y = np.eye(10)[y_test]\n",
        "print(\"Size of training labels:\", train_y.shape)\n",
        "print(\"Size of test labels:\", test_y.shape)\n",
        "\n",
        "no_classes=10  #0 to 9 digits\n",
        "\n",
        "#one vs all approach\n",
        "digits=[]\n",
        "for i in range(no_classes):\n",
        "  lgr=logistic_regression(train_x.shape[0])\n",
        "  print(\"\\nTraining class {} vs others.....\".format(i))\n",
        "  lgr.train_and_optimize(train_x, train_y[:,i])\n",
        "  acc=lgr.predict(test_x, test_y[:,i])\n",
        "  print(\"\\nAccuracy of predicting {} is: {}\".format(i, acc))\n",
        "  digits.append(lgr)\n",
        "  "
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of training data: (784, 60000)\n",
            "Size of test data: (784, 10000)\n",
            "Size of training labels: (60000, 10)\n",
            "Size of test labels: (10000, 10)\n",
            "\n",
            "Training class 0 vs others.....\n",
            "No of training examples: 60000\n",
            "Cost after 0 iterations: 0.0019527282537008409\n",
            "Cost after 25 iterations: 0.0009505785556437984\n",
            "Cost after 49 iterations: 0.0007615880902489908\n",
            "\n",
            "Accuracy of predicting 0 is: 96.63000000000001\n",
            "\n",
            "Training class 1 vs others.....\n",
            "No of training examples: 60000\n",
            "Cost after 0 iterations: 0.0018913011604768917\n",
            "Cost after 25 iterations: 0.0008404737109978187\n",
            "Cost after 49 iterations: 0.0006834958551378328\n",
            "\n",
            "Accuracy of predicting 1 is: 96.14\n",
            "\n",
            "Training class 2 vs others.....\n",
            "No of training examples: 60000\n",
            "Cost after 0 iterations: 0.0019491655442262683\n",
            "Cost after 25 iterations: 0.001080593295927213\n",
            "Cost after 49 iterations: 0.0009426050514178988\n",
            "\n",
            "Accuracy of predicting 2 is: 91.16\n",
            "\n",
            "Training class 3 vs others.....\n",
            "No of training examples: 60000\n",
            "Cost after 0 iterations: 0.0019530424540908497\n",
            "Cost after 25 iterations: 0.0010884523685211803\n",
            "Cost after 49 iterations: 0.0009513587638792738\n",
            "\n",
            "Accuracy of predicting 3 is: 91.71000000000001\n",
            "\n",
            "Training class 4 vs others.....\n",
            "No of training examples: 60000\n",
            "Cost after 0 iterations: 0.0019236564478829063\n",
            "Cost after 25 iterations: 0.001002570679046535\n",
            "Cost after 49 iterations: 0.0008721800580068196\n",
            "\n",
            "Accuracy of predicting 4 is: 90.84\n",
            "\n",
            "Training class 5 vs others.....\n",
            "No of training examples: 60000\n",
            "Cost after 0 iterations: 0.0019197629102113956\n",
            "Cost after 25 iterations: 0.0010399977552210656\n",
            "Cost after 49 iterations: 0.0009438401292999213\n",
            "\n",
            "Accuracy of predicting 5 is: 91.08000000000001\n",
            "\n",
            "Training class 6 vs others.....\n",
            "No of training examples: 60000\n",
            "Cost after 0 iterations: 0.0019398981052199445\n",
            "Cost after 25 iterations: 0.0010152802589759825\n",
            "Cost after 49 iterations: 0.0008557807078504572\n",
            "\n",
            "Accuracy of predicting 6 is: 93.11\n",
            "\n",
            "Training class 7 vs others.....\n",
            "No of training examples: 60000\n",
            "Cost after 0 iterations: 0.001925049760348919\n",
            "Cost after 25 iterations: 0.0009681374060044239\n",
            "Cost after 49 iterations: 0.0008146852787100644\n",
            "\n",
            "Accuracy of predicting 7 is: 93.84\n",
            "\n",
            "Training class 8 vs others.....\n",
            "No of training examples: 60000\n",
            "Cost after 0 iterations: 0.0019692692761425932\n",
            "Cost after 25 iterations: 0.0012159598019369302\n",
            "Cost after 49 iterations: 0.0011228008239435952\n",
            "\n",
            "Accuracy of predicting 8 is: 90.25999999999999\n",
            "\n",
            "Training class 9 vs others.....\n",
            "No of training examples: 60000\n",
            "Cost after 0 iterations: 0.0019397474311890872\n",
            "Cost after 25 iterations: 0.0010914618922287318\n",
            "Cost after 49 iterations: 0.0009859711333400202\n",
            "\n",
            "Accuracy of predicting 9 is: 89.91\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZDuLaQ2m0-X8",
        "colab_type": "code",
        "outputId": "f6844d9b-b3da-40b6-fcfd-dfaeb9a4679e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 670
        }
      },
      "cell_type": "code",
      "source": [
        "No_to_predict = input(\"Enter any image from 0 to 60000 for prediction: \")\n",
        "\n",
        "plt.imshow(train_x[:,int(No_to_predict)].reshape((28,28)))\n",
        "predictions=[]\n",
        "\n",
        "for i in range(no_classes):\n",
        "  lgr=digits[i]\n",
        "  prob=lgr.calc_probability(train_x[:,int(No_to_predict)])\n",
        "  print(\"\\nProbability of the number being {} is: {}\".format(i, prob))\n",
        "  predictions.append(prob)\n",
        "  \n",
        "preds=np.asarray(predictions)\n",
        "print(\"\\nPredicted number is:\",np.argmax(preds))\n",
        "  "
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter any image from 0 to 60000 for prediction: 89\n",
            "\n",
            "Probability of the number being 0 is: [0.09987021]\n",
            "\n",
            "Probability of the number being 1 is: [0.02532742]\n",
            "\n",
            "Probability of the number being 2 is: [0.0619659]\n",
            "\n",
            "Probability of the number being 3 is: [0.01925372]\n",
            "\n",
            "Probability of the number being 4 is: [0.44686549]\n",
            "\n",
            "Probability of the number being 5 is: [0.08961578]\n",
            "\n",
            "Probability of the number being 6 is: [0.12372778]\n",
            "\n",
            "Probability of the number being 7 is: [0.16839566]\n",
            "\n",
            "Probability of the number being 8 is: [0.10901798]\n",
            "\n",
            "Probability of the number being 9 is: [0.22483158]\n",
            "\n",
            "Predicted number is: 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAD4CAYAAADFJPs2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhTFh98Cn8QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}